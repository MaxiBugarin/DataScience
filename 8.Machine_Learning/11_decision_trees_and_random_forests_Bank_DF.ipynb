{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_decision_trees_and_random_forests Bank DF.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/al34n1x/DataScience/blob/master/99.Machine_Learning/11_decision_trees_and_random_forests_Bank_DF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfzdslHeWFcp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# Decision Trees y Random Forests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBXqV_QuWFcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBx1ovXHWFcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbKqzKByWFcv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "En este notebook vamos a aprender a utilizar Árboles de Decisión en Scikit-learn.\n",
        "Como veremos, los árboles de decisión son extremadamente intiutivos. En el fondo codifican una serie de decisiones de la misma manera que lo haría una sentencia condicional *if-else*. La diferencia es que, en lugar de programar estas decisiones manualmente, el árbol aprende a generarlas automáticamente en función de los datos de entrenamiento.\n",
        "Por ejemplo, si queremos utilizar una guía para identificar animales, podríamos preguntar las siguientes preguntas:\n",
        "\n",
        "- Este animal, mide más de un metro de longitud?\n",
        "    + *Más de 1m long*. ¿Tiene cuernos?\n",
        "        - *Sí*: ¿Miden sus cuernos más de 10cm de longitud?\n",
        "        - *No*: ¿El animal tiene pelo o escamas?\n",
        "    + *Menos de 1m *: ¿El animal tiene 2 o 4 patas?\n",
        "        - *Dos patas*: ¿Tiene alas?\n",
        "        - *Cuatro patas*: ¿Tiene cola de más de 12 cm de largo?\n",
        "  \n",
        "Y así podría seguir... La esencia de un árbol de decisión es ir realizando preguntas que permitan hacer particiones binarias del dataset.\n",
        "\n",
        "Una de las principales ventajas de los árboles es que necesitan muy poco pre-procesado (p.ej. son invariantes al escalado de las variables) y permiten trabajar con diferentes tipos de variables (contínuas, discretas). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN2Kxi-IWFcv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Decision Trees: Regresión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jm7ZHlnWFcw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Vamos a empezar con un ejemplo de árboles de Regresión. Para ello crearemos un árbol de decisión que intentará ajustar los datos sinténticos que vamos a crear a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY4bNMyJWFcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_dataset(n_samples=100):\n",
        "    rnd = np.random.RandomState(42)\n",
        "    x = np.linspace(-3, 3, n_samples)\n",
        "    y_no_noise = np.sin(4 * x) + x\n",
        "    y = y_no_noise + rnd.normal(size=len(x))\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiY4DXP2WFcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = make_dataset()\n",
        "X = x.reshape(-1, 1)\n",
        "\n",
        "plt.xlabel('Feature X')\n",
        "plt.ylabel('Target y')\n",
        "plt.scatter(X, y);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHdu3DR0WFc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "reg = DecisionTreeRegressor(max_depth=5)\n",
        "reg.fit(X, y)\n",
        "\n",
        "X_fit = np.linspace(-3, 3, 1000).reshape((-1, 1))\n",
        "y_fit_1 = reg.predict(X_fit)\n",
        "\n",
        "plt.plot(X_fit.ravel(), y_fit_1, color='navy', label=\"prediction\")\n",
        "plt.plot(X.ravel(), y, '.k', label=\"training data\")\n",
        "plt.legend(loc=\"best\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FBImG9kWFc5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "##### Como vemos, el modelo genera zonas con líneas planas muy anchas (bias) y zonas con cambios verticales muy bruscos ajustándose a uno o muy pocos puntos (overfitting).\n",
        "\n",
        "Esto es uno de los problemas típicos de los árboles que se puede controlar con la profundidad del mismo. \n",
        "\n",
        "## Ejercicio\n",
        "\n",
        "- Probar varios parámetros de profundidad y observar como aumentan el bias y la varianza del modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOofm2urWFc6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# Decision Tree: Clasificación\n",
        "\n",
        "Los problemas de clasificación también se pueden resolver árboles de decisión.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VycoGO2cYunu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmMr5ytr2rw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importo el CSV creado previamente\n",
        "\n",
        "df2 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/UADE/Diplomatura Ciencia de Datos/Material de Clases/MESES_201402_201404.csv', dtype={\"numero_de_cliente\": int, \"foto_mes\": int, \"cliente_sucursal\": \"string\"})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXr7WpCS7A4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaSsFFAM5jb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Elimino la primer columna\n",
        "df2 = df2.loc[:, ~df2.columns.str.contains('^Unnamed')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J8Ty06ws94w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2.describe(include='all')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEIRhpd83GuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YueWg3IX3VR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWwOGn9Ny-im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df2.drop(['target' , 'target_bin'] , axis='columns')\n",
        "y = df2.target_bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uBltjMj41zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "X = X.select_dtypes(include=numerics)\n",
        "X = X.fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxpWRwVr5K7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxmf367l_W1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = df2.target_bin\n",
        "counts = s.value_counts()\n",
        "percent = s.value_counts(normalize=True)\n",
        "percent100 = s.value_counts(normalize=True).mul(100).round(2).astype(str) + '%'\n",
        "pd.DataFrame({'counts': counts, 'per': percent, 'per100': percent100})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXJLpWd4BarH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Entreno un Arbol de Decición\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "# Train model\n",
        "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "#clf = DecisionTreeClassifier(max_depth=5).fit(X_train, y_train)\n",
        "#clf = DecisionTreeClassifier(max_depth=5, class_weight='balanced').fit(X_train, y_train)\n",
        " \n",
        "# Predict on training set and test set\n",
        "y_train_pred = clf.predict(X_train)\n",
        "y_test_pred = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biDoh6ACxC1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Accuracy \n",
        "from sklearn.metrics import accuracy_score\n",
        "print('Accuracy train:',accuracy_score(y_train, y_train_pred))\n",
        "print('Accuracy test:',accuracy_score(y_test, y_test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn0MZcq3CF9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# veo la matriz de confusión en train\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_train, y_train_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDrI78huJJUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# veo la matriz de confusión en test\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLlxKvGLyf1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Ahora puedo obtener Recall y Precision\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "print('Accuracy')\n",
        "print('Train:',accuracy_score(y_train, y_train_pred))\n",
        "print('Test:',accuracy_score(y_test, y_test_pred))\n",
        "print(' ')\n",
        "print('Recall')\n",
        "print('Train:',recall_score(y_train, y_train_pred))\n",
        "print('Test:',recall_score(y_test, y_test_pred))\n",
        "print(' ')\n",
        "print('Precision')\n",
        "print('Train:',precision_score(y_train, y_train_pred))\n",
        "print('Test:',precision_score(y_test, y_test_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaVgoUBUv9rj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import average_precision_score\n",
        "average_precision = average_precision_score(y_test, y_test_pred)\n",
        "\n",
        "print('Average precision-recall score: {0:0.2f}'.format(\n",
        "      average_precision))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_O23JTSvlPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "disp = plot_precision_recall_curve(clf, X_test, y_test)\n",
        "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
        "                   'AP={0:0.2f}'.format(average_precision))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj4GDCckoime",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    print()\n",
        "    print(format('How to plot a ROC Curve in Python','*^82'))\n",
        "\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    # load libraries\n",
        "    from sklearn.datasets import make_classification\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.metrics import roc_curve, roc_auc_score\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Create feature matrix and target vector\n",
        "    #X, y = make_classification(n_samples=10000, n_features=100, n_classes=2)\n",
        "\n",
        "    # Split into training and test sets\n",
        "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
        "\n",
        "    # Create classifier\n",
        "    clf1 = DecisionTreeClassifier()\n",
        "    clf2 = DecisionTreeClassifier(max_depth=5)\n",
        "    clf3 = LogisticRegression()\n",
        "\n",
        "    # Train model\n",
        "    clf1.fit(X_train, y_train)\n",
        "    clf2.fit(X_train, y_train)\n",
        "    clf3.fit(X_train, y_train)\n",
        "\n",
        "    # Get predicted probabilities\n",
        "    y_score1 = clf1.predict_proba(X_test)[:,1]\n",
        "    y_score2 = clf2.predict_proba(X_test)[:,1]\n",
        "    y_score3 = clf3.predict_proba(X_test)[:,1]\n",
        "\n",
        "    # Plot Receiving Operating Characteristic Curve\n",
        "    # Create true and false positive rates\n",
        "    false_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_test, y_score1)\n",
        "    false_positive_rate2, true_positive_rate2, threshold2 = roc_curve(y_test, y_score2)\n",
        "    false_positive_rate3, true_positive_rate3, threshold3 = roc_curve(y_test, y_score3)\n",
        "    print('roc_auc_score for DecisionTree: ', roc_auc_score(y_test, y_score1))\n",
        "    print('roc_auc_score for DecisionTree_5: ', roc_auc_score(y_test, y_score2))\n",
        "    print('roc_auc_score for Logistic Regression: ', roc_auc_score(y_test, y_score3))\n",
        "\n",
        "    # Plot ROC curves\n",
        "    plt.subplots(1, figsize=(10,10))\n",
        "    plt.title('Receiver Operating Characteristic - DecisionTree')\n",
        "    plt.plot(false_positive_rate1, true_positive_rate1)\n",
        "    plt.plot([0, 1], ls=\"--\")\n",
        "    plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()\n",
        "\n",
        "    plt.subplots(1, figsize=(10,10))\n",
        "    plt.title('Receiver Operating Characteristic - DecisionTree Levels 5')\n",
        "    plt.plot(false_positive_rate2, true_positive_rate2)\n",
        "    plt.plot([0, 1], ls=\"--\")\n",
        "    plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()\n",
        "\n",
        "    plt.subplots(1, figsize=(10,10))\n",
        "    plt.title('Receiver Operating Characteristic - Logistic regression')\n",
        "    plt.plot(false_positive_rate3, true_positive_rate3)\n",
        "    plt.plot([0, 1], ls=\"--\")\n",
        "    plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD1LtHwOgBj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## import dependencies\n",
        "from sklearn import tree #For our Decision Tree\n",
        "import pandas as pd # For our DataFrame\n",
        "import pydotplus # To create our Decision Tree Graph\n",
        "from IPython.display import Image  # To Display a image of our graph\n",
        "\n",
        "\n",
        "# The decision tree classifier.\n",
        "#clf2 = tree.DecisionTreeClassifier(max_depth=5)\n",
        "clf2 = tree.DecisionTreeClassifier()\n",
        "# Training the Decision Tree\n",
        "clf_train = clf2.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Export/Print a decision tree in DOT format.\n",
        "#print(tree.export_graphviz(clf_train, None))\n",
        "\n",
        "#Create Dot Data\n",
        "dot_data = tree.export_graphviz(clf_train, out_file=None, feature_names=list(X_train.columns.values), \n",
        "                                class_names=['0', '1'], rounded=True, filled=True) #Gini decides which attribute/feature should be placed at the root node, which features will act as internal nodes or leaf nodes\n",
        "#Create Graph from DOT data\n",
        "graph = pydotplus.graph_from_dot_data(dot_data)\n",
        "\n",
        "# Show graph\n",
        "Image(graph.create_png())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsUGRpPczWBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Entreno un Arbol de Decición con Cross Validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "cross_val_score(clf, X, y, cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOpVpML_8vB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Veo los parametros\n",
        "clf.predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkIOSwusWFdH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        " ## Ejercicio\n",
        "\n",
        "Entrenar el modelo árboles más profundos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNy6z9k0WFdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Respuesta aqui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO0Z8gviWFdK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6eXCz-HWFdK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Random Forests son ensamblajes de muchos árboles entrenados con diferentes subconjuntos de datos y diferentes subconjuntos de variables del dataset de entrenamiento. \n",
        "Esto permite que los árboles del bosque sean diferentes entre ellos. De hecho, cada árbol se especializa en un aspecto diferente del dataset (sobreajusta al subconjunto de datos con los que se ha entrenado ese árbol), pero como la predicción es un promedio de las predicciones de todos los árboles, al final el bosque es capaz de reducir la varianza (overfitting)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyNiLjyQWFdL",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Seleccionando los parámetros de un bosque utilizando Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTv2npPwWFdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=50) #200\n",
        "parameters = {'max_features':['sqrt', 'log2', 10],\n",
        "              'max_depth':[5, 7, 9]}\n",
        "\n",
        "clf_grid = GridSearchCV(rf, parameters, n_jobs=-1)\n",
        "clf_grid.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjKRpgJkWFdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf_grid.score(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K09HR9fY1z65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict on training set and test set\n",
        "y_train_pred = clf_grid.predict(X_train)\n",
        "y_test_pred = clf_grid.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLKrmbL00nZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Accuracy')\n",
        "print('Train:',accuracy_score(y_train, y_train_pred))\n",
        "print('Test:',accuracy_score(y_test, y_test_pred))\n",
        "print(' ')\n",
        "print('Recall')\n",
        "print('Train:',recall_score(y_train, y_train_pred))\n",
        "print('Test:',recall_score(y_test, y_test_pred))\n",
        "print(' ')\n",
        "print('Precision')\n",
        "print('Train:',precision_score(y_train, y_train_pred))\n",
        "print('Test:',precision_score(y_test, y_test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDqGDEreWFdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf_grid.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pezjZf0-WFdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf_grid.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCdERNuXWFdY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Gradient Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCZGZnOiWFdZ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Otro método de ensemble es *Boosting*. Aquí, en lugar de construir 200 estimadores en paralelo (como hacíamos en el bosque anterior), construimos una cadena de 200 estimadores que iterativamente van refinando los resultados del estimador anterior. La idea de boosting es que, utilizar modelos muy simples y rápidos secuencialmente nos permite obtener un error menor que el que pueden obtener los distintos estimadores individualmente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjakUKMxWFdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "clf = GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=.2)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(clf.score(X_train, y_train))\n",
        "print(clf.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA-x9PBBWFdh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Ejercicio: Gradient Boosting con cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpLDVZF1WFdi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "- Utilizad una búsqueda en grid para optimizar los parámetros `learning_rate` (0.01-1) y `max_depth` (1-9) de un Gradient Boosted Trees en el dataset de Bank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFQTwcecWFdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVPS3GEoWFdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Respuesta aqui"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}