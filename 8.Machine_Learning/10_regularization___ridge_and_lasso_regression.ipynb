{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "pygments_lexer": "ipython3",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "nbconvert_exporter": "python",
      "version": "3.7.0",
      "mimetype": "text/x-python",
      "name": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "10_regularization_-_ridge_and_lasso_regression.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/al34n1x/DataScience/blob/master/8.Machine_Learning/10_regularization___ridge_and_lasso_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX4Xappa3s0z"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDmzJz-X3s08"
      },
      "source": [
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIjdmdPq3s1D"
      },
      "source": [
        "\n",
        "\n",
        "# Regularización\n",
        "\n",
        "En este notebook vamos a estudiar como la regularización puede ayudarnos a reducir los efectos del bias y la varianza. En concreto veremos como utilizar regularización en modelos de regresión.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6MCKwOw3s1E"
      },
      "source": [
        "\n",
        "\n",
        "# Recordando regresión\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdx88-kz3s1F"
      },
      "source": [
        "\n",
        "\n",
        "Todos los modelos de regresión lineal aprenden una serie de *coeficientes* (también llamados parámetros o pesos) y un *intercept* (también llamado bias) para realizar una combinación lineal de las características del modelo a aprender. Así tenemos que:\n",
        "\n",
        "```\n",
        "y_pred = x_test[0] * coef_[0] + ... + x_test[n_features-1] * coef_[n_features-1] + intercept_\n",
        "```\n",
        "\n",
        "Existen varios modelos de regresión lineal cuya principal diferencia reside en las restricciones que le ponemos en los *coeficientes*. Recordad que estas restricciones se conocen como regularización.\n",
        "\n",
        "Vamos a generar una conjunto de datos sintético que nos permita ver el comportamiento de estos modelos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFjX7GwC3s1G"
      },
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y, true_coefficient = make_regression(n_samples=200, n_features=30,\n",
        "                                         n_informative=10, noise=100,\n",
        "                                         coef=True, random_state=5)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5, train_size=60)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRbqVGuQ3s1M"
      },
      "source": [
        "\n",
        "\n",
        "## Linear Regression\n",
        "\n",
        "El modelo lineal más famoso es la regresión OLS (Ordinary Least Squares), llamada *regresión lineal*. Este tipo de regresión no impone ninguna restricción a los coeficientes. Esto significa que cuando tenemos un gran número de caracterísiticas, el modelo tienda a sobreajustar.\n",
        "\n",
        "$$ \\text{min}_{w, b} \\sum_i || w^\\mathsf{T}x_i + b  - y_i||^2 $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra0OoI343s1N"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "linear_regression = LinearRegression().fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZw5mL0f3s1U"
      },
      "source": [
        "print(\"R^2 on training set: %f\" % linear_regression.score(X_train, y_train))\n",
        "print(\"R^2 on test set: %f\" % linear_regression.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7E2QDdR3s1Z"
      },
      "source": [
        "\n",
        "\n",
        "Como vemos el $R^2$ en el conjunto de entrenamiento es muy alto, pero muy bajo en el conjunto de test. Esto suele ser un indicativo de que nuestro modelo está sobreajustando."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJtCFBUe3s1b"
      },
      "source": [
        "\n",
        "\n",
        "Vamos a aprovecharnos que nuestros datos son sinténticos para evaluar como se comporta el modelo real sobre los datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDRqiLie3s1c"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "print(r2_score(np.dot(X, true_coefficient), y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEga8PRX3s1f"
      },
      "source": [
        "\n",
        "\n",
        "Por lo que podemos decir que un $R^2 \\approx 0.6$ es un resultado ideal para nuestro conjunto de datos. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6qSKZgE3s1h"
      },
      "source": [
        "\n",
        "\n",
        "También vamos a aprovechar que tenemos conjuntos de datos sinténticos para comparar los coeficientes del aprendidos por el modelo con los coeficientes reales de nuestros datos sinténticos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgewXCdA3s1i"
      },
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "coefficient_sorting = np.argsort(true_coefficient)[::-1]\n",
        "plt.plot(true_coefficient[coefficient_sorting], \"o\", label=\"true\")\n",
        "plt.plot(linear_regression.coef_[coefficient_sorting], \"o\", label=\"linear regression\")\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVS2jUwS3s1n"
      },
      "source": [
        "\n",
        "\n",
        "Un método muy útil para conocer si nuestro modelo sufre de *underfitting* o de *overfitting* es calcular las curvas de aprendizaje. Éstas muestran el score del modelo en los conjuntos de training y de test para un número determinado de muestras de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SJ9tliI3s1o"
      },
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "def plot_learning_curve(est, X, y):\n",
        "    training_set_size, train_scores, test_scores = learning_curve(est, X, y, train_sizes=np.linspace(.1, 1, 20))\n",
        "    estimator_name = est.__class__.__name__\n",
        "    line = plt.plot(training_set_size, train_scores.mean(axis=1), '--', label=\"training scores \" + estimator_name)\n",
        "    plt.plot(training_set_size, test_scores.mean(axis=1), '-', label=\"test scores \" + estimator_name, c=line[0].get_color())\n",
        "    plt.xlabel('Training set size')\n",
        "    plt.ylabel('R² Score')\n",
        "    plt.legend(loc='best')\n",
        "    plt.ylim(-0.1, 1.1)\n",
        "\n",
        "plt.figure(figsize=(12,8))    \n",
        "plot_learning_curve(LinearRegression(), X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcNLMF3r3s1u"
      },
      "source": [
        "\n",
        "\n",
        "## Ridge Regression (L2 penalty)\n",
        "\n",
        "La **Regresión de Ridge** es un método de regularización que impone una restricción a los pesos del modelo de regresión simple. La regresión que impone la Regresión de Ridge se basa en la norma $L2$. Este tipo de regularización tiene la ventaja de no ser mucho más pesada computacionalmente que una regresión por OLS.\n",
        "\n",
        "$$ \\text{min}_{w,b}  \\sum_i || w^\\mathsf{T}x_i + b  - y_i||^2  + \\alpha ||w||_2^2$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIKI7n_r3s1v"
      },
      "source": [
        "\n",
        "\n",
        "Como vemos, la Regresión de Ridge, es muy parecida a la Regresión Lineal. La única diferencia entre ambas es que Ridge añade la norma L2 a la función de minimización.\n",
        "\n",
        "El parámetro libre $\\alpha$ regula la cantidad de regularización que queemos aplicar al modelo. Vamos a probar varios valores de $\\alpha$ para evaluar como afecta al performance del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQMHASUS3s1w"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "ridge_models = {}\n",
        "training_scores = []\n",
        "test_scores = []\n",
        "alphas = [100, 30, 10, 5, 1, 0.1, .01, 0.001]\n",
        "for alpha in alphas:\n",
        "    ridge = Ridge(alpha=alpha).fit(X_train, y_train)\n",
        "    training_scores.append(ridge.score(X_train, y_train))\n",
        "    test_scores.append(ridge.score(X_test, y_test))\n",
        "    ridge_models[alpha] = ridge\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(training_scores, label=\"training scores\")\n",
        "plt.plot(test_scores, label=\"test scores\")\n",
        "plt.xticks(range(len(alphas)), alphas)\n",
        "plt.legend(loc=\"best\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D0ugBo73s10"
      },
      "source": [
        "\n",
        "\n",
        "Tenemos el máximo score en el conjunto de test para $\\alpha = 10$.\n",
        "\n",
        "\n",
        "**EJERCICIO**:\n",
        "\n",
        "- Qué le ocurre al modelo para valores de $\\alpha$ menores a 10?\n",
        "- Qué le ocurre al modelo par avalores de $\\alpha$ mayores a 10?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1OYoajP3s12"
      },
      "source": [
        "\n",
        "\n",
        "Vamos a ver lo ocurre con los coeficientes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyfDtvvf3s13"
      },
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(true_coefficient[coefficient_sorting], \"s\", label=\"true\", c='navy')\n",
        "\n",
        "for i, alpha in enumerate([100, 10, 1, .01]):\n",
        "    plt.plot(ridge_models[alpha].coef_[coefficient_sorting], \"o\", label=\"alpha = %.2f\" % alpha, alpha=0.8, c=plt.cm.summer(i / 3.))\n",
        "    \n",
        "plt.legend(loc=\"best\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QwtRTEx3s18"
      },
      "source": [
        "\n",
        "\n",
        "- Valores altos de $\\alpha$ producen que el modelo tienda a ajustar los pesos a 0.\n",
        "  - Es decir, aumentan el bias del modelo.\n",
        "- Valores bajos de $\\alpha$ producen que el modelo se comporte como una regresión lineal sin regularización."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QL6WQFl3s19"
      },
      "source": [
        "\n",
        "\n",
        "En **resumen**: es muy importante elegir el valor óptimo de $\\alpha$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kCARry53s1-"
      },
      "source": [
        "\n",
        "\n",
        "Ahora vamos a ver las curvas de aprendizaje del modelo de Ridge vs la Regresión Lineal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xgEZdyK3s1-"
      },
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plot_learning_curve(LinearRegression(), X, y)\n",
        "plot_learning_curve(Ridge(alpha=10), X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmzE5mQO3s2D"
      },
      "source": [
        "\n",
        "\n",
        "## Lasso (penalty L1)\n",
        "\n",
        "La **Regresión de Lasso** es otro tipo de regularización pero que impone una restrucción utilizando la norma $L1$. Este tipo de regularización es útil cuando creemos que muchas de los parámetros del modelo no son relevantes (como es el caso de nuestro dataset). \n",
        "\n",
        "$$ \\text{min}_{w, b} \\sum_i || w^\\mathsf{T}x_i + b  - y_i||^2  + \\alpha ||w||_1$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTMjF5JS3s2E"
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "lasso_models = {}\n",
        "training_scores = []\n",
        "test_scores = []\n",
        "\n",
        "alphas = [100, 30, 10, 5, 1, 0.1, .01, 0.001]\n",
        "for alpha in alphas:\n",
        "    lasso = Lasso(alpha=alpha).fit(X_train, y_train)\n",
        "    training_scores.append(lasso.score(X_train, y_train))\n",
        "    test_scores.append(lasso.score(X_test, y_test))\n",
        "    lasso_models[alpha] = lasso\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(training_scores, label=\"training scores\")\n",
        "plt.plot(test_scores, label=\"test scores\")\n",
        "plt.xticks(range(len(alphas)), alphas)\n",
        "plt.legend(loc=\"best\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yJVeAEq3s2L"
      },
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(true_coefficient[coefficient_sorting], \"s\", label=\"true\", c='navy')\n",
        "\n",
        "for i, alpha in enumerate([30, 10, 1, .01]):\n",
        "    plt.plot(lasso_models[alpha].coef_[coefficient_sorting], \"o\", label=\"alpha = %.2f\" % alpha, alpha=0.9, c=plt.cm.summer(i / 3.))\n",
        "    \n",
        "plt.legend(loc=\"best\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw4Omcmr3s2P"
      },
      "source": [
        "plt.figure(figsize=(12, 8))    \n",
        "plot_learning_curve(LinearRegression(), X, y)\n",
        "plot_learning_curve(Ridge(alpha=10), X, y)\n",
        "plot_learning_curve(Lasso(alpha=10), X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHgmCbXD3s2U"
      },
      "source": [
        "\n",
        "\n",
        "**Nota**: En lugar de tener que elegir entre Ridge o Lasso, es posible utilizar la Regresión [ElasticNet](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html), que combina ambas formas de regularización a través de un parámetro de ponderación que nos permite elegir a qué tipo de regularización darle más peso. En muchos casos ElasticNet obtiene un rendimiento mayor que Ridge o Lasso.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-qnvoD-3s2V"
      },
      "source": [
        "\n",
        "\n",
        "# Ejercicio\n",
        "\n",
        "Hasta ahora hemos mostrado las *learning curves* para un valor de $\\alpha = 10$ ya que habíamos visto que era el valor óptimo para nuestros datos. Ahora vamos a probar que ocurre en las learning curves si las modificamos.\n",
        "\n",
        "1. Escoje un valor de $\\alpha < 10$ y muestra las learning curves (para Lasso y Ridge). ¿Qué ocurre?\n",
        "2. Escoje un valor de $\\alpha > 10$ y muestra las learning curves (para Lasso y Ridge). ¿Qué ocurre?\n",
        "   1. ¿Como mejoraríais el resultado del modelo para Lasso?\n",
        "   2. ¿Y para Ridge?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROBMEjhy3s2V"
      },
      "source": [
        "# Respuesta aqui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m541OnmF3s2X"
      },
      "source": [
        "\n",
        "\n",
        "- $\\alpha$ valores pequeños: el modelo ejecuta una regresión lineal\n",
        "- $\\alpha$ valores grandes: el modelo tiende al sobreajuste \n",
        "- Mejora del modelo Lasso: reduce $\\alpha$ o incrementa la complejidad del modelo (añade valores útiles).\n",
        "- Mejora del modelo Ridge: puede ser realizada solamente como el modelo Lasso. Sin embargo, es también posible añadir más valores (el modelo tiene una tendencia creciente).\n"
      ]
    }
  ]
}