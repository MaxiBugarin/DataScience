{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO3msAwF4Qgi3NLS4F5T1E+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/al34n1x/DataScience/blob/master/9.BigData/Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "HYuz1sZL9PiI"
      },
      "source": [
        ">[Spark](#updateTitle=true&folderId=1hYY6URNFLa2w5I3uQbpDlwOox_am-5cM&scrollTo=v46kRpi27oRc)\n",
        "\n",
        ">>[Instalación en Colab](#updateTitle=true&folderId=1hYY6URNFLa2w5I3uQbpDlwOox_am-5cM&scrollTo=JIGw62oM7ypl)\n",
        "\n",
        ">>[Variables de entorno](#updateTitle=true&folderId=1hYY6URNFLa2w5I3uQbpDlwOox_am-5cM&scrollTo=Pld58Eng79T6)\n",
        "\n",
        ">>[Iniciar sesión de Spark](#updateTitle=true&folderId=1hYY6URNFLa2w5I3uQbpDlwOox_am-5cM&scrollTo=u6IA7l5A8Y5v)\n",
        "\n",
        ">>[Carga de archivos](#updateTitle=true&folderId=1hYY6URNFLa2w5I3uQbpDlwOox_am-5cM&scrollTo=IyyTuVe38h1W)\n",
        "\n",
        ">>[Almacenar archivo en formato Parquet](#updateTitle=true&folderId=1hYY6URNFLa2w5I3uQbpDlwOox_am-5cM&scrollTo=2nLwFkPD8w30)\n",
        "\n",
        ">>[Spark SQL](#updateTitle=true&folderId=1hYY6URNFLa2w5I3uQbpDlwOox_am-5cM&scrollTo=PSRPJafR8_85)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v46kRpi27oRc"
      },
      "source": [
        "# Spark\n",
        "\n",
        "Apache Spark es un motor unificado diseñado para el procesamiento de datos distribuidos a gran escala, en centros de datos o en la nube.\n",
        "La filosofía de diseño de Spark se centra en cuatro características clave:\n",
        "* Velocidad\n",
        "* Facilidad de uso\n",
        "* Modularidad\n",
        "* Extensibilidad\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIGw62oM7ypl"
      },
      "source": [
        "## Instalación en Colab\n",
        "\n",
        "Primero instalamos los paquetes necesarios\n",
        "\n",
        "* OpenJDK\n",
        "* Spark con Hadoop\n",
        "* findspark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODRcAiIpodZU"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pld58Eng79T6"
      },
      "source": [
        "## Variables de entorno\n",
        "\n",
        "Al ser un sandbox, debemos de configurar variables de entorno que serán utilizadas por Spark.\n",
        "A continuación detallamos las dos variables de entornos requeridas, una para **JAVA** y otra para **SPARK**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUIL2IZeoe1M"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.8-bin-hadoop2.7\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6IA7l5A8Y5v"
      },
      "source": [
        "## Iniciar sesión de Spark\n",
        "\n",
        "Una vez instalado los paquetes y configurado las variabls de entorno, podemos pasar a iniciar con el uso de Spark.\n",
        "Las siguientes lineas nos permitirán iniciar la sesión de spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YTeGJZkogUl"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyyTuVe38h1W"
      },
      "source": [
        "## Carga de archivos\n",
        "\n",
        "Como lo hemos comentado, Spark se nutre de diferentes fuentes de datos, en nuestro caso, para el alcance del ejemplo, tomaremos uno de los archivos **CSV** dispuestos en COLAB para luego poder ser almacenado en Hadoop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKQ2fS8f5cmV"
      },
      "source": [
        "archivo = './sample_data/california_housing_train.csv'\n",
        "df_spark = spark.read.csv(archivo, inferSchema=True, header=True)\n",
        "print(type(df_spark))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXTF4YYY5jII"
      },
      "source": [
        "df_spark.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X_LSNAY5mgH"
      },
      "source": [
        "df_spark.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYlnca4u5pkc"
      },
      "source": [
        "df_spark.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdcTz5ak5sK1"
      },
      "source": [
        "df_spark.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaAfi8NF5xpO"
      },
      "source": [
        "df_spark.describe().toPandas().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nLwFkPD8w30"
      },
      "source": [
        "## Almacenar archivo en formato Parquet\n",
        "\n",
        "En esta etapa cambiamos el DF a formato Parquet para poder crear vistas temporales que nos servirán para poder realizar consultas con **Spark SQL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRl2rE9f6RbE"
      },
      "source": [
        "df_spark.write.parquet(\"california_housing_train.parquet\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcMvUO9j6bWP"
      },
      "source": [
        "parquetFile = spark.read.parquet(\"california_housing_train.parquet\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnp1Hsz965GI"
      },
      "source": [
        "parquetFile.createOrReplaceTempView(\"parquetFile\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSRPJafR8_85"
      },
      "source": [
        "## Spark SQL\n",
        "\n",
        "A continuación desarrollamos la consulta SQL que deseamos realizar a la fuente de datos y Spark se encargará de traernos la información."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3H390ec68uG"
      },
      "source": [
        "california = spark.sql(\"SELECT * FROM parquetFile where median_income < 1.4\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WikeSTs7G11"
      },
      "source": [
        "california.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}